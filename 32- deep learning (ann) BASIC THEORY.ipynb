{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING - ARTIFICIAL NEURAL NETWORK\n",
    "\n",
    "    METHOD TO MIMIC HOW HUMAN BRAIN\n",
    "    IN BRAIN THERE ARE 100 BILLION NEURONS\n",
    "   \n",
    "   \n",
    "## GOD FATHER OF DEEP LEARNING IS GEOFFREY HINTON\n",
    "\n",
    "\n",
    "THERE'S A INPUT LAYER A HIDDEN LAYER AND A OUTPUT LAYER TO PRODUCE THE O/P. THE MAIN PART OF THE NEURAL NETWORK IS HIDDEN LAYER WHICH DOES ALL THE WORK \n",
    "\n",
    "## THE HUMAN NEURON \n",
    "    \n",
    "     THIS IS THE BASIC BUILDING BLOCK OF NEURAL NETWORK \n",
    "     \n",
    "     HUMAN NEURON HAS 3 PARTS :\n",
    "                               1. DENDRITES- RECEIVER OF THE SIGNAL.\n",
    "                               2. NEURON - HELPS CHOOSE THE RECEIVER\n",
    "                               3. AXON - TRANSMITTER OF THE SIGNAL FOR THE NEURON\n",
    "                               \n",
    "    LINES CONNECTING THE 2 NEURONS IN THE REAL HUMAN BRAIN AND THE ARTIFICAL NEURAL NETWORK IS CALLED SYNAPSE.\n",
    "    \n",
    "WEIGHTS ARE THE MOST IMPORTANT PART OF THE NEURAL NETWORK AND THE WEIGHTS SELECT THE STRENGTH OF THE SIGNALS COMING IN AND DURING TRAINING OF THE NERUAL NETWORK WEIGHTS ARE THE ONE GETING UPDATED.\n",
    "\n",
    "## WHAT  HAPPENS INSIDE THE NEURON ?\n",
    "       \n",
    "       1. WEIGHTS AND THE INPUT VALUES GETS ADDED UP.\n",
    "       2. ACTIVATION FUNCTION IS APPLIED TO THE O/P OBTAINED BY THE STEP 1.\n",
    "       \n",
    "## ACTIVATION FUNCTION\n",
    "\n",
    "    THERE ARE 4 MAIN TYPE OF ACTIVATION FUNCTION.\n",
    "           1. THRESHOLD FUNCTION:  Q(x)=1 if x>=0\n",
    "                                       =0 if x<0\n",
    "           2. SIGMOID FUNCTION: Q(x)=1/(1+exp(-z))\n",
    "           \n",
    "           3. RECTIFIER FUNCTION: Q(x)= max(x,0)\n",
    "           \n",
    "           4. HYPERBOLIC TANGENT FUNCTION(tanh): Q(x)=(1-exp(-2x))/(1+exp(-2x))\n",
    "\n",
    "## PERCEPTRON - IT IS A SINGLE HIDDEN LAYER NEURAL NETWORK\n",
    "\n",
    "   I/P IS GIVEN AND THEN O/P IS CALCULATED BY APPLYING ACTIVATION FUNCTION. THE O/P GENERATED IS COMPARED TO ACTUAL VALUE AND COST FUNCTION IS CALCULATED. THIS COST FUNCTION IS FEED BACK TO THE NETWORK AND BY BACK PROPOGATION WEIGHTS ARE UPDATED.\n",
    "   \n",
    "## 1 EPOCH\n",
    "         UPDATION THE WEIGHTS ONCE IS CALLED 1 EPOCH\n",
    "         \n",
    "## GRADIENT DESCENT\n",
    "     \n",
    "     METHOD OF REACHING THE GLOBAL MINIMA ON THE COST FUNCTION\n",
    "      THERE ARE 3 MAIN TYPES OF GRADIENT DESCENT :\n",
    "                1. BATCH GRADIENT DESCENT : COST CALCULATED AT THE END OF BATCH AND WEIGHTS UPDATED WHEN THE BATCH GETS OVER. WORKS ONLY FOR CONVEX COST FUNCTION.\n",
    "                2. STOCHASTIC GRADIENT DESCENT : FOR NON CONVEX FUNCTION IN WHICH WE CAN LAND ON LOCAL MINIMA RATHER THAN LANDING ON GLOBAL MINIMA. HENCE TO PREVENT THIS EACH ROW TAKEN AND EACH ROW UPDATES THE WEIGHT.\n",
    "                3. MINI BATCH GRADIENT DESCENT : MIX OF BOTH BATCH AND STOCHASTIC GRADINET DESCENT.\n",
    "                \n",
    "## BACKPROPOGATION\n",
    "\n",
    "    THE GOAL OF BACK PROPOGATION IS TO COMPUTE THE PARTIAL DERIVATIVE OF COST FUNCTION WRT WEIGHTS (W) AND BIAS (B)\n",
    "       IN A NUTSHELL BACKPROPOGATION IS CHAIN RULE USED IN PARTIAL DERIVATIVE.\n",
    "       \n",
    "### Function Composition\n",
    "\n",
    "#### g ( f ( x ) ) \n",
    "- If these functions are linear, result will be linear\n",
    "- If these functions are non-linear, the degree of polynomial will keep increasing       \n",
    "\n",
    "### Neural Neworks = Logistic Regressions chained on top of each other\n",
    "\n",
    "#### Activation Functions\n",
    "- Non-linear functions in this context are called Activation Functions\n",
    "- Logistic Function (Sigmoid) is the activation here.\n",
    "\n",
    "#### Logistic Regression --(S)---\n",
    "\n",
    "where\n",
    "- W'X (Transpose) --( )--\n",
    "- Sigmoid --S--\n",
    "\n",
    "\n",
    "##### --[W1]--[S]--[W2]--[S]---\n",
    "\n",
    "1. Linear: Y0 = W1'X \n",
    "2. Logistic: Y1 = sig(Y0)\n",
    "\n",
    "3. Linear Y2 = W2'Y1\n",
    "4. Logistic: sig(Y2)\n",
    "\n",
    "\n",
    "#### Universal Function Approximator\n",
    "\n",
    "If the last node in chain is:\n",
    "- Linear  then NeuralNet will do regression. Ex: y = x\n",
    "- --(S)--(S)--(/)--\n",
    "- Probabilistic then NeuralNet will do classification. Ex: y = sig(x)\n",
    "- --(S)--(S)--(S)--\n",
    "\n",
    "\n",
    "Last node depends on the task. Classification vs Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solver (optimizer)\n",
    "- sgd: stochastic gradient descent (Update rule uses only the 1-st derivative)\n",
    "- adam: similar to sgd\n",
    "- lbfgs: quasi-Newton methods (Also computes 2nd derivative. Slow. Refer: Hessian Matrix)\n",
    "\n",
    "\n",
    "Loss/Error\n",
    "- MSE: Mean squared error (used with linear)\n",
    "- BCE: Binary cross entropy (used with sigmoid)\n",
    "- CCE: Categorical cross entropy (used with softmax)\n",
    "\n",
    "\n",
    "Activation\n",
    "We can use any activation function as long as it is non-linear\n",
    "- Logistic or Sigmoid\n",
    "- ReLU (Rectified Linear Unit) = max(0, x)\n",
    "- tanh\n",
    "- identity (This is linear!)\n",
    "\n",
    "Softmax\n",
    "- Generalized Sigmoid for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Higher level library on top of \n",
    "- TensorFlow\n",
    "- Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry\n",
    "\n",
    "When you don't have the features (70% times)\n",
    "- Use Neural Nets becuase they also extract features from data\n",
    "- Example of unstructured data: MNIST \n",
    "\n",
    "When you have features (30% times)\n",
    "- Random Decision forest\n",
    "- XGBoost Classifier\n",
    "- Example of structured data: Titanic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

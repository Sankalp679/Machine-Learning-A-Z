{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplistic implementation of the two-layer neural network.\n",
    "Training method is stochastic (online) gradient descent with momentum.\n",
    "As an example it computes XOR for given input.\n",
    "Some details:\n",
    "- tanh activation for hidden layer\n",
    "- sigmoid activation for output layer \"\"\"\n",
    "#dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "n_hidden=10  # neurons in 1 hidden layer \n",
    "n_input=10   # input layer \n",
    "n_output=10   # output layer\n",
    "n_sample=300  # no. of sample\n",
    "\n",
    "# hyperparameter\n",
    "learning_rate=0.01\n",
    "momentum=0.9\n",
    "\n",
    "# seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):          # activation function 1\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_prime(x):      # activation function 2\n",
    "    return 1- np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters accepted are data input,data target,layer1,layer2,bias layer 1 , bias layer 2\n",
    "def train(x,t,L1,L2,b1,b2):\n",
    "    # forward propogation\n",
    "    a=np.dot(x,L1)+b1    # layer 1\n",
    "    z=np.tanh(a)\n",
    "    b=np.dot(z,L2)+b2    #layer 2\n",
    "    y=sigmoid(b)\n",
    "    # backward propoagation\n",
    "    el2=y-t         # error in L2 layer \n",
    "    el1=tanh_prime(a)*np.dot(el2,L2)   #error in L1 layer\n",
    "    dl2=np.outer(z,el2)                # differentiation of error in L2\n",
    "    dl1=np.outer(x,el1)                # of L1\n",
    "    # here we are doing classification and thus we use cross entropy as are error/ cost function\n",
    "    loss=-np.mean ( t * np.log(y) + (1 - t) * np.log(1 - y) )\n",
    "    return loss,(dl1,dl2,el1,el2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,L1,L2,b1,b2):\n",
    "    a = np.dot(x, L1) + b1             #passing through 1st hidden layer\n",
    "    b = np.dot(np.tanh(a), L2) + b2    #passing through 2nd hidden layer\n",
    "    return (sigmoid(b)>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:\t 0 \n",
      "loss:\t 0.479751776883375\n",
      "epochs:\t 1 \n",
      "loss:\t 0.12218161840381417\n",
      "epochs:\t 2 \n",
      "loss:\t 0.053902778762558716\n",
      "epochs:\t 3 \n",
      "loss:\t 0.031164888095424104\n",
      "epochs:\t 4 \n",
      "loss:\t 0.02173115005649793\n",
      "epochs:\t 5 \n",
      "loss:\t 0.016646473795091963\n",
      "epochs:\t 6 \n",
      "loss:\t 0.013470905246095477\n",
      "epochs:\t 7 \n",
      "loss:\t 0.011289144199811717\n",
      "epochs:\t 8 \n",
      "loss:\t 0.009693600920549758\n",
      "epochs:\t 9 \n",
      "loss:\t 0.008475037117921467\n",
      "epochs:\t 10 \n",
      "loss:\t 0.007514050407517927\n",
      "epochs:\t 11 \n",
      "loss:\t 0.006737139143237239\n",
      "epochs:\t 12 \n",
      "loss:\t 0.0060964508219048886\n",
      "epochs:\t 13 \n",
      "loss:\t 0.0055594476591042195\n",
      "epochs:\t 14 \n",
      "loss:\t 0.00510321268319299\n",
      "epochs:\t 15 \n",
      "loss:\t 0.004711124795634085\n",
      "epochs:\t 16 \n",
      "loss:\t 0.0043708261564136845\n",
      "epochs:\t 17 \n",
      "loss:\t 0.004072931620825234\n",
      "epochs:\t 18 \n",
      "loss:\t 0.0038101822197661285\n",
      "epochs:\t 19 \n",
      "loss:\t 0.003576873798301845\n",
      "epochs:\t 20 \n",
      "loss:\t 0.003368461471862667\n",
      "epochs:\t 21 \n",
      "loss:\t 0.003181279560158236\n",
      "epochs:\t 22 \n",
      "loss:\t 0.0030123392783200234\n",
      "epochs:\t 23 \n",
      "loss:\t 0.002859179976181966\n",
      "epochs:\t 24 \n",
      "loss:\t 0.0027197580077489517\n",
      "epochs:\t 25 \n",
      "loss:\t 0.002592362529803392\n",
      "epochs:\t 26 \n",
      "loss:\t 0.0024755508888507473\n",
      "epochs:\t 27 \n",
      "loss:\t 0.0023680984675035\n",
      "epochs:\t 28 \n",
      "loss:\t 0.0022689593468316047\n",
      "epochs:\t 29 \n",
      "loss:\t 0.002177235157216999\n",
      "epochs:\t 30 \n",
      "loss:\t 0.002092150196898278\n",
      "epochs:\t 31 \n",
      "loss:\t 0.002013031396418931\n",
      "epochs:\t 32 \n",
      "loss:\t 0.0019392920645307777\n",
      "epochs:\t 33 \n",
      "loss:\t 0.001870418610256558\n",
      "epochs:\t 34 \n",
      "loss:\t 0.001805959625966429\n",
      "epochs:\t 35 \n",
      "loss:\t 0.001745516857353503\n",
      "epochs:\t 36 \n",
      "loss:\t 0.0016887376918388042\n",
      "epochs:\t 37 \n",
      "loss:\t 0.00163530887680947\n",
      "epochs:\t 38 \n",
      "loss:\t 0.0015849512400054827\n",
      "epochs:\t 39 \n",
      "loss:\t 0.001537415231197203\n",
      "epochs:\t 40 \n",
      "loss:\t 0.001492477140568176\n",
      "epochs:\t 41 \n",
      "loss:\t 0.0014499358775147584\n",
      "epochs:\t 42 \n",
      "loss:\t 0.0014096102157977162\n",
      "epochs:\t 43 \n",
      "loss:\t 0.0013713364285453875\n",
      "epochs:\t 44 \n",
      "loss:\t 0.0013349662505732747\n",
      "epochs:\t 45 \n",
      "loss:\t 0.0013003651166518834\n",
      "epochs:\t 46 \n",
      "loss:\t 0.0012674106333318302\n",
      "epochs:\t 47 \n",
      "loss:\t 0.0012359912491894305\n",
      "epochs:\t 48 \n",
      "loss:\t 0.001206005094246174\n",
      "epochs:\t 49 \n",
      "loss:\t 0.001177358964120647\n",
      "epochs:\t 50 \n",
      "loss:\t 0.0011499674284087562\n",
      "epochs:\t 51 \n",
      "loss:\t 0.0011237520460278428\n",
      "epochs:\t 52 \n",
      "loss:\t 0.0010986406729372965\n",
      "epochs:\t 53 \n",
      "loss:\t 0.0010745668498682503\n",
      "epochs:\t 54 \n",
      "loss:\t 0.0010514692595434327\n",
      "epochs:\t 55 \n",
      "loss:\t 0.0010292912444124843\n",
      "epochs:\t 56 \n",
      "loss:\t 0.0010079803772227375\n",
      "epochs:\t 57 \n",
      "loss:\t 0.0009874880778343723\n",
      "epochs:\t 58 \n",
      "loss:\t 0.0009677692706078452\n",
      "epochs:\t 59 \n",
      "loss:\t 0.0009487820774691772\n",
      "epochs:\t 60 \n",
      "loss:\t 0.0009304875424189012\n",
      "epochs:\t 61 \n",
      "loss:\t 0.0009128493838125082\n",
      "epochs:\t 62 \n",
      "loss:\t 0.0008958337712200681\n",
      "epochs:\t 63 \n",
      "loss:\t 0.0008794091240832366\n",
      "epochs:\t 64 \n",
      "loss:\t 0.0008635459297404067\n",
      "epochs:\t 65 \n",
      "loss:\t 0.0008482165786936888\n",
      "epochs:\t 66 \n",
      "loss:\t 0.0008333952152530092\n",
      "epochs:\t 67 \n",
      "loss:\t 0.0008190576019182954\n",
      "epochs:\t 68 \n",
      "loss:\t 0.0008051809960566628\n",
      "epochs:\t 69 \n",
      "loss:\t 0.0007917440376012445\n",
      "epochs:\t 70 \n",
      "loss:\t 0.0007787266466461083\n",
      "epochs:\t 71 \n",
      "loss:\t 0.0007661099299406914\n",
      "epochs:\t 72 \n",
      "loss:\t 0.0007538760953996104\n",
      "epochs:\t 73 \n",
      "loss:\t 0.0007420083738422278\n",
      "epochs:\t 74 \n",
      "loss:\t 0.0007304909472627902\n",
      "epochs:\t 75 \n",
      "loss:\t 0.0007193088830077765\n",
      "epochs:\t 76 \n",
      "loss:\t 0.0007084480733038602\n",
      "epochs:\t 77 \n",
      "loss:\t 0.0006978951796387771\n",
      "epochs:\t 78 \n",
      "loss:\t 0.0006876375815493814\n",
      "epochs:\t 79 \n",
      "loss:\t 0.0006776633294170587\n",
      "epochs:\t 80 \n",
      "loss:\t 0.0006679611009115128\n",
      "epochs:\t 81 \n",
      "loss:\t 0.0006585201607599652\n",
      "epochs:\t 82 \n",
      "loss:\t 0.0006493303235509881\n",
      "epochs:\t 83 \n",
      "loss:\t 0.0006403819193106998\n",
      "epochs:\t 84 \n",
      "loss:\t 0.0006316657616146175\n",
      "epochs:\t 85 \n",
      "loss:\t 0.0006231731180210614\n",
      "epochs:\t 86 \n",
      "loss:\t 0.0006148956826324262\n",
      "epochs:\t 87 \n",
      "loss:\t 0.0006068255506087254\n",
      "epochs:\t 88 \n",
      "loss:\t 0.0005989551944742\n",
      "epochs:\t 89 \n",
      "loss:\t 0.0005912774420722669\n",
      "epochs:\t 90 \n",
      "loss:\t 0.00058378545603732\n",
      "epochs:\t 91 \n",
      "loss:\t 0.000576472714663665\n",
      "epochs:\t 92 \n",
      "loss:\t 0.0005693329940624719\n",
      "epochs:\t 93 \n",
      "loss:\t 0.0005623603515072614\n",
      "epochs:\t 94 \n",
      "loss:\t 0.0005555491098770676\n",
      "epochs:\t 95 \n",
      "loss:\t 0.0005488938431142194\n",
      "epochs:\t 96 \n",
      "loss:\t 0.0005423893626208228\n",
      "epochs:\t 97 \n",
      "loss:\t 0.0005360307045243305\n",
      "epochs:\t 98 \n",
      "loss:\t 0.0005298131177484479\n",
      "epochs:\t 99 \n",
      "loss:\t 0.0005237320528309441\n",
      "XOR prediction: [0 0 0 1 1 1 0 0 0 1]\n",
      "[1 1 1 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# assigning initial values to the parameters \n",
    "L1 = np.random.normal(scale=0.1, size=(n_input, n_hidden))           #layer 1 \n",
    "L2 = np.random.normal(scale=0.1, size=(n_hidden, n_output))          #layer 2\n",
    "\n",
    "b1=np.zeros(n_hidden)          # bias for layer 1\n",
    "b2=np.zeros(n_output)          # bias for layer 2\n",
    "\n",
    "# for our convience\n",
    "parameter=[L1,L2,b1,b2]\n",
    "\n",
    "# generating random dataset\n",
    "x=np.random.binomial(1,0.5,(n_sample,n_input))\n",
    "t=x^1\n",
    "\n",
    "\n",
    "# now training the dataset on the model created\n",
    "for epoch in range(100):\n",
    "    err = []\n",
    "    upd = [0]*len(parameter)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        loss, grad = train(x[i], t[i], *parameter)\n",
    "\n",
    "        for j in range(len(parameter)):\n",
    "            parameter[j] -= upd[j]\n",
    "\n",
    "        for j in range(len(parameter)):\n",
    "            upd[j] = learning_rate * grad[j] + momentum * upd[j]\n",
    "\n",
    "        err.append( loss )\n",
    "\n",
    "    print(\"epochs:\\t\",epoch,\"\\nloss:\\t\",np.mean(err))\n",
    "\n",
    "\n",
    "# testing our model on the test dataset\n",
    "x = np.random.binomial(1, 0.5, n_input)\n",
    "print (\"XOR prediction:\",x)\n",
    "print (predict(x, *parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

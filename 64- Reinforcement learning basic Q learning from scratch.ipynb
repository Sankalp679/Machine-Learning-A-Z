{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gym) (1.16.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: six in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: future in c:\\users\\hseth\\anaconda3\\envs\\tf_gpu\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initiating environment in Ai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for discrete action space game \n",
    "env_name=\"CartPole-v1\"\n",
    "# creating environment\n",
    "env=gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)  # states in the environment used \n",
    "\n",
    "# in this case there are 4 \n",
    "# cart position , cart velocity , pole angle , pole velocity at tip\n",
    "\n",
    "print(env.action_space)   # action that can be taken by AI\n",
    "\n",
    "# in this case there are 2 actions that can be taken \n",
    "# right and left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic code for environment access / use the gym\n",
    "# resting variable\n",
    "state = env.reset()\n",
    "for _ in range(200):\n",
    "    #action = random.choice(range(action_space var))  # this line and line below it are same\n",
    "    action = env.action_space.sample()   # sample works as random and chooses from action_space randomly\n",
    "    state, reward, done, info = env.step(action) #action determined used to take a step in the game \n",
    "    env.render()    #render means to display \n",
    "\n",
    "env.close()      # to automatically close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making action little bit smarter using tilt value of the pole\n",
    "state = env.reset()\n",
    "for _ in range(200):\n",
    "    pole_angle=state[2]    # using pole_angle in state variable  # known as policy\n",
    "    action = 0 if pole_angle<0 else 1         #known as policy\n",
    "    state,reward,done,info = env.step(action)\n",
    "    env.render()\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for continuous action space \n",
    "env_name=\"MountainCarContinuous-v0\"\n",
    "env=gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "Box(1,)\n",
      "[1.]\n",
      "[-1.]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "# car position and velocity\n",
    "print(env.action_space)\n",
    "# push car left or to right range(-1 to+1)\n",
    "print(env.action_space.high) # to see range \n",
    "print(env.action_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for _ in range(200):\n",
    "    action=env.action_space.sample()\n",
    "    state,reward,done,info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (Q-Learning)\n",
    "\n",
    "\n",
    "# Basic Algo\n",
    "   \n",
    "   # Q(st,at) = rt+1 +G.Qmax(st+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n game board :\\n \\n SFFF       // S = starting point\\n FHFH       // F = forzen surface,Safe\\n FFFH       // H = hole , fall not safe\\n HFFG       // G = goal, finish point\\n \\n actions :\\n       \\n       left =0\\n       right=2\\n       up   =3\\n       down =1\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working on toy text game of frozen lake provided by ai gym \n",
    "'''\n",
    " game board :\n",
    " \n",
    " SFFF        S = starting point\n",
    " FHFH        F = forzen surface,Safe\n",
    " FFFH        H = hole , fall not safe\n",
    " HFFG        G = goal, finish point\n",
    " \n",
    " actions :\n",
    "       \n",
    "       left =0\n",
    "       right=2\n",
    "       up   =3\n",
    "       down =1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from gym.envs.registration import register  # to use register function\n",
    "from IPython.display import clear_output  # feature of jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register is in the init function of the source code of the open ai\n",
    "try:     #used as if we register one game again we get error therfore to avoid we use exception handling\n",
    "    register(\n",
    "        id='FrozenLakenoslip-v0',     # we can change this name if we want before (-v0)\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4','is_slippery':False},  # making game non slippery for our benifit\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "env_name = \"FrozenLakenoslip-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)    # 4X4 matrix\n",
    "print(env.action_space)     # 4 movement\n",
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using object orientation\n",
    "class Agent():                     # player class\n",
    "    def __init__(self, env):      # constructor  # self is Agent\n",
    "        \n",
    "# checking if the env we are working with is discrete or continuous \n",
    "        self.is_discrete = type(env.action_space)==gym.spaces.discrete.Discrete   # if true: is_discrete= True, else:False\n",
    "        \n",
    "        if self.is_discrete:                          # if true then size is below\n",
    "            self.action_size = env.action_space.n\n",
    "        else:                                        # else store max and min value\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            \n",
    "# to figure out which action to use next all logic will be here \n",
    "    def get_action(self,state):\n",
    "        # action = env.action_space.sample()     #different approch than line written beneath this\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size)) # just for testing\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,   #just for testing\n",
    "                                       self.action_high,\n",
    "                                       self.action_shape)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)  # class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "for episode in range(2):  # for 2 episodes\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        time.sleep(0.5)          #time wait before redering different o/p and clean pre o/p\n",
    "        clear_output(wait=True)  # to clear previous o/p\n",
    "        #no close as will render on screen only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating smart / Qlearning based agent using about agent \n",
    "# concept of inheritance\n",
    "\n",
    "class QAgent(Agent):       # inheritance\n",
    "    def __init__(self, env, discount_rate=0.97, learning_rate=0.01):\n",
    "        \n",
    "        super().__init__(env)  #getting init of Agent class inheritance concept\n",
    "        \n",
    "        self.state_size = env.observation_space.n\n",
    "        \n",
    "        self.exploration_rate = 1.0  # to not be stuck with same path and try and find different path (adds randomness)\n",
    "        # exploration for 1st episode is max and then after every episode exploration keeps changing\n",
    "        # represented by epsilon\n",
    "        \n",
    "        self.discount_rate=discount_rate  # represented by gamma\n",
    "        self.learning_rate=learning_rate  # represented by alpha\n",
    "        \n",
    "        self.q_table = 0.0001 * np.random.random([self.state_size,self.action_size])  \n",
    "        # matrix formed of row x column states rows and actions column\n",
    "        #0.0001 can also be written as 1e-4 used to reduce the action values\n",
    "        \n",
    "        \n",
    "    def get_action(self,state):\n",
    "        q_state = self.q_table[state] # select row from q table based on state from 0 to 15 in this state\n",
    "        action_greedy = np.argmax(q_state)  #gives index of max value in qtable corresponding to state \n",
    "        action_random = super().get_action(state)  # for exploration use random state \n",
    "        return action_random if random.random()<self.exploration_rate else action_greedy # to explore randomly and in other cases return true action\n",
    "    \n",
    "    def train(self,experience):\n",
    "        state, action, next_state, reward, done = experience  #extracting values from experience of 1 episode\n",
    "        \n",
    "        q_next = self.q_table[next_state]  # getting the next states q value as we need it to calculate new q_value in algo\n",
    "        q_next = np.zeros([self.action_size]) if done else q_next #if agent reaches goal no next state will be there and if not use above value\n",
    "        q_target = reward + self.discount_rate * np.max(q_next) # q_next algorithm coded here\n",
    "        q_update=q_target-self.q_table[state,action] # differnence b/w calculated and previous value in table\n",
    "        \n",
    "        self.q_table[state,action]+=self.learning_rate*q_update   #updating the qvalues in small amount therefore mul by learning rate\n",
    "        \n",
    "        if done:      # after each episode decrease the exploration rate so that it can follow right path \n",
    "            self.exploration_rate*=0.99\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=QAgent(env)  #object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s15 a:2\n",
      "epsiode:199 reward:116.0 exploration:0.04904089407128576\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[5.49447499e-05 4.53613713e-05 1.32360029e-03 5.65187574e-05]\n",
      " [4.13352852e-05 2.38359620e-05 7.05226731e-03 4.13905180e-05]\n",
      " [1.12356287e-04 3.09460399e-02 5.87019937e-05 2.85607448e-04]\n",
      " [1.56814379e-04 3.94884488e-05 8.73487872e-06 8.26218614e-05]\n",
      " [4.66217303e-05 3.32680351e-05 1.09687851e-05 4.16313978e-05]\n",
      " [5.95914952e-05 8.78806179e-05 5.58948810e-05 1.60469870e-06]\n",
      " [6.09885998e-05 1.14251312e-01 6.09929013e-05 1.56005595e-04]\n",
      " [2.73354065e-05 2.06371139e-06 8.32926573e-05 6.37699391e-05]\n",
      " [7.17111763e-05 6.13522603e-05 1.29028922e-05 5.13282604e-05]\n",
      " [5.52703716e-05 4.74807619e-04 5.00006456e-05 2.83630322e-05]\n",
      " [2.48989027e-05 3.28638812e-01 3.56934739e-05 9.27739672e-04]\n",
      " [7.51192552e-06 6.96172234e-05 1.44946209e-05 1.74544317e-05]\n",
      " [1.77753573e-06 4.21372847e-05 4.19616648e-05 4.15631286e-05]\n",
      " [7.36181173e-05 9.05051403e-05 2.15465021e-02 3.98559492e-05]\n",
      " [2.30058507e-04 1.73331340e-02 7.03626266e-01 1.71253926e-03]\n",
      " [3.53535288e-06 3.86168276e-05 2.92468777e-05 1.40163688e-05]]\n"
     ]
    }
   ],
   "source": [
    "total_reward=0\n",
    "for episode in range(200):  # for 2 episodes\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train((state,action,next_state,reward,done))    # calling train function and passing tuple in the exploration variable\n",
    "        state=next_state # updating state \n",
    "        total_reward+=reward       # checking the reward\n",
    "        print(f\"s{state} a:{action}\")   \n",
    "        print(f\"epsiode:{episode} reward:{total_reward} exploration:{agent.exploration_rate}\")\n",
    "        \n",
    "        env.render()\n",
    "        print(agent.q_table)\n",
    "        time.sleep(0.05)          #time wait before redering different o/p and clean pre o/p\n",
    "        clear_output(wait=True)  # to clear previous o/p\n",
    "                #no close as will required as displaying in notebook only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

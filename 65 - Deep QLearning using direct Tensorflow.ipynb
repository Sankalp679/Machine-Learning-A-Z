{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gym.envs.registration import register  # to use register function\n",
    "from IPython.display import clear_output  # feature of jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register is in the init function of the source code of the open ai\n",
    "try:     #used as if we register one game again we get error therfore to avoid we use exception handling\n",
    "    register(\n",
    "        id='FrozenLakenoslip-v0',     # we can change this name if we want before (-v0)\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4','is_slippery':False},  # making game non slippery for our benifit\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "env_name = \"FrozenLakenoslip-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)    # 4X4 matrix\n",
    "print(env.action_space)     # 4 movement\n",
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using object orientation\n",
    "class Agent():                     # player class\n",
    "    def __init__(self, env):      # constructor  # self is Agent\n",
    "        \n",
    "# checking if the env we are working with is discrete or continuous \n",
    "        self.is_discrete = type(env.action_space)==gym.spaces.discrete.Discrete   # if true: is_discrete= True, else:False\n",
    "        \n",
    "        if self.is_discrete:                          # if true then size is below\n",
    "            self.action_size = env.action_space.n\n",
    "        else:                                        # else store max and min value\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            \n",
    "# to figure out which action to use next all logic will be here \n",
    "    def get_action(self,state):\n",
    "        # action = env.action_space.sample()     #different approch than line written beneath this\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size)) # just for testing\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,   #just for testing\n",
    "                                       self.action_high,\n",
    "                                       self.action_shape)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNAgent(Agent):\n",
    "    def __init__(self,env,discount_rate=0.97,learning_rate=0.01):\n",
    "        \n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n # for discrete\n",
    "        self.exploration_rate = 1.0 \n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()   # for tensorflow code\n",
    "        \n",
    "        # to run/start session the model we have built in tensorflow we write bellow code\n",
    "        self.sess=tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def build_model(self):    # tensorflow model here\n",
    "        tf.reset_default_graph() # to reset all names given to layer\n",
    "        \n",
    "        # tensorflow needs to know what variable type is \n",
    "        # tf.placeholder used to pass/ create variable type and in bracket type and shape is given\n",
    "        self.state_in = tf.placeholder(tf.int32, shape=[1]) # input state for our model\n",
    "        self.action_in = tf.placeholder(tf.int32, shape=[1]) # action input state for our model\n",
    "        self.target_in = tf.placeholder(tf.float32, shape=[1]) #  target input state for our model needs to be float\n",
    "        \n",
    "        self.state = tf.one_hot(self.state_in,depth=self.state_size)  # one hot encoding state_in value and saving it in state\n",
    "        # depth gives the size of state_in that is 16 in this case \n",
    "        self.action = tf.one_hot(self.action_in,depth=self.action_size)\n",
    "        # depth here is 4 as there are 4 possible actions\n",
    "        \n",
    "        self.q_state = tf.layers.dense(self.state,units=self.action_size,name=\"q_table\")\n",
    "        #saving the layer in variable q_state\n",
    "        self.q_action = tf.reduce_sum(tf.multiply(self.q_state,self.action),axis=1)\n",
    "        # multiplying actual o/p with layers o/p\n",
    "        \n",
    "        self.loss=tf.reduce_sum(tf.square(self.target_in - self.q_action))  # squaring and adding to get loss\n",
    "        self.optimizer=tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        # choosing the optimizer and using optimizer to reduce loss\n",
    "        \n",
    "    def get_action(self,state):\n",
    "        q_state = self.sess.run(self.q_state,feed_dict={self.state_in: [state]}) # no back prop only prediction\n",
    "        # the above line code ,1 iteration of model with inputs given as dict using feed_dict variable\n",
    "        # now q_states hold the o/p given by layers \n",
    "        action_greedy = np.argmax(q_state)  # learned state\n",
    "        action_random = super().get_action(state) # for exploration choose random action\n",
    "        return action_random if random.random()<self.exploration_rate else action_greedy\n",
    "    \n",
    "    def train(self,experience):\n",
    "        state,action,next_state,reward,done = [[exp] for exp in experience]\n",
    "        # list comprehension done as in tensorflow we need to pass list in the model so we are creating each element as list\n",
    "        #now we have made the list therefore no need to pass list below\n",
    "        q_next = self.sess.run(self.q_state, feed_dict={self.state_in:next_state})  # no back prop only prediction\n",
    "        #running the model again here as we need next state q value in qtable algo \n",
    "        q_next[done]= np.zeros(self.action_size) #if done else q_next  # if game done\n",
    "        q_target = reward+self.discount_rate*np.max(q_next)\n",
    "        \n",
    "        # creating a dic for the optimizer that we are using in our model\n",
    "        feed={self.state_in:state,self.action_in:action,self.target_in:q_target}\n",
    "        \n",
    "        # updating optimizer using below lines \n",
    "        # this is actual training part and backprop performed here \n",
    "        self.sess.run(self.optimizer,feed_dict=feed)\n",
    "        \n",
    "        if done[0]:\n",
    "            self.exploration_rate*=0.99\n",
    "    \n",
    "    def __del__self(self):   #deconstructor\n",
    "        sess.sess.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000237E4934088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000237E4934088>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000237E4934088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000237E4934088>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "agent=QNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s15 a:2\n",
      "epsiode:99 reward:69.0 exploration:0.13397967485796175\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[ 0.15231162  0.05854299  0.19316824  0.0736348 ]\n",
      " [ 0.19942246 -0.7356729   0.21187428  0.12320637]\n",
      " [ 0.19301312  0.1141359   0.22164726  0.14607859]\n",
      " [ 0.20927079 -0.60846865  0.02380594  0.03808787]\n",
      " [ 0.14336185  0.0681117  -0.6003821   0.07825181]\n",
      " [-0.2017557  -0.42708766  0.1829803   0.18884534]\n",
      " [-0.6758391   0.1637468  -0.25909626  0.05128361]\n",
      " [ 0.24932957 -0.48201343 -0.30184117  0.17893845]\n",
      " [ 0.20239714 -0.7607962   0.25232655  0.10861719]\n",
      " [ 0.21751364  0.10430083  0.28718358 -0.8155135 ]\n",
      " [ 0.21618131  0.16236055 -0.55861443  0.16362707]\n",
      " [-0.3943591   0.2185747  -0.52882516  0.34483308]\n",
      " [ 0.1621362  -0.37694463 -0.14190015 -0.47672582]\n",
      " [-0.29023674 -0.015889    0.24226455  0.17535181]\n",
      " [ 0.2164892   0.14758724  0.35221773  0.19689049]\n",
      " [-0.12469646  0.22696102 -0.43533072  0.27271748]]\n"
     ]
    }
   ],
   "source": [
    "total_reward=0.0\n",
    "for ep in range(100):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    while not done:\n",
    "        action=agent.get_action(state)\n",
    "        next_state,reward,done,info = env.step(action)\n",
    "        agent.train((state,action,next_state,reward,done))\n",
    "        state=next_state\n",
    "        total_reward+=reward\n",
    "        print(f\"s{state} a:{action}\")   \n",
    "        print(f\"epsiode:{ep} reward:{total_reward} exploration:{agent.exploration_rate}\")\n",
    "        \n",
    "        env.render()\n",
    "        #the statement below are used to prevent the q_table to be reset for each iteration\n",
    "        with tf.variable_scope(\"q_table\",reuse=True):  #set reuse true so that we can use it again\n",
    "            weights=agent.sess.run(tf.get_variable(\"kernel\")) #to get q_table \n",
    "            print(weights)\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience replay timing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

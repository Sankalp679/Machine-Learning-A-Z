{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gym.envs.registration import register \n",
    "from IPython.display import clear_output  # feature of jupyter notebook\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEQUE  ?\n",
    "Deque can be implemented in python using the module “collections“. Deque is preferred over list in the cases where we need quicker append and pop operations from both the ends of container, as deque provides an O(1) time complexity for append and pop operations as compared to list which provides O(n) time complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI Gym: 0.17.1\n",
      "Using Tensorflow: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Using OpenAI Gym:\", gym.__version__)\n",
    "print(\"Using Tensorflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register is in the init function of the source code of the open ai\n",
    "try:     #used as if we register one game again we get error therfore to avoid we use exception handling\n",
    "    register(\n",
    "        id='FrozenLakenoslip-v0',     # we can change this name if we want before (-v0)\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4','is_slippery':False},  # making game non slippery for our benifit\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "env_name = \"FrozenLakenoslip-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "Discrete(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gym.spaces.discrete.Discrete"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(env.observation_space)    # 4X4 matrix\n",
    "print(env.action_space)     # 4 movement\n",
    "type(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        self.is_discrete = \\\n",
    "            type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        \n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "            print(\"Action size:\", self.action_size)\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "            print(\"Action range:\", self.action_low, self.action_high)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = random.choice(range(self.action_size))\n",
    "        else:\n",
    "            action = np.random.uniform(self.action_low,\n",
    "                                       self.action_high,\n",
    "                                       self.action_shape)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 4\n",
      "State size: 16\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000023551760BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000023551760BC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000023551760BC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000023551760BC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# same code as normal neural net only differnce is buffer made using deque\n",
    "class QNRAgent(Agent):\n",
    "    def __init__(self, env, discount_rate=0.97, learning_rate=0.001): # Smaller learning rate\n",
    "        super().__init__(env)\n",
    "        self.state_size = env.observation_space.n\n",
    "        print(\"State size:\", self.state_size)\n",
    "        \n",
    "        self.eps = 1.0\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.build_model()\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.replay_buffer = deque(maxlen=1000) # buffer deque of size max len 1000\n",
    "        \n",
    "    def build_model(self):\n",
    "        tf.reset_default_graph()\n",
    "        self.state_in = tf.placeholder(tf.int32, shape=[None]) # None means any shape\n",
    "        self.action_in = tf.placeholder(tf.int32, shape=[None]) #\n",
    "        self.target_in = tf.placeholder(tf.float32, shape=[None]) #\n",
    "        \n",
    "        self.state = tf.one_hot(self.state_in, depth=self.state_size)\n",
    "        self.action = tf.one_hot(self.action_in, depth=self.action_size)\n",
    "        \n",
    "        self.q_state = tf.layers.dense(self.state, units=self.action_size, name=\"q_table\")\n",
    "        self.q_action = tf.reduce_sum(tf.multiply(self.q_state, self.action), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.square(self.target_in - self.q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        q_state = self.sess.run(self.q_state, feed_dict={self.state_in: [state]})\n",
    "        action_greedy = np.argmax(q_state)\n",
    "        action_random = super().get_action(state)\n",
    "        return action_random if random.random() < self.eps else action_greedy\n",
    "    \n",
    "    def train(self, experience, batch_size=50):\n",
    "        self.replay_buffer.append(experience) # append buffer with experience\n",
    "        samples = random.choices(self.replay_buffer, k=batch_size) # randomly choose the experience and return list of size 50\n",
    "        state, action, next_state, reward, done = (list(col) for col in zip(experience, *samples))\n",
    "        # The purpose of zip() is to map the similar index of multiple containers so that they can be used just using as single entity\n",
    "        # *samples is used to pass all items in the list \n",
    "        \n",
    "        q_next = self.sess.run(self.q_state, feed_dict={self.state_in: next_state})\n",
    "        q_next[done] = np.zeros([self.action_size])\n",
    "        q_target = reward + self.discount_rate * np.max(q_next, axis=1)\n",
    "        \n",
    "        feed = {self.state_in: state, self.action_in: action, self.target_in: q_target}\n",
    "        self.sess.run(self.optimizer, feed_dict=feed)\n",
    "        \n",
    "        if experience[4]:\n",
    "            self.eps = self.eps * 0.99\n",
    "            \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "agent = QNRAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: 15 a: 2\n",
      "Episode: 99, Total reward: 98.0, eps: 0.017950553275045134\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[ 0.5293679   0.41516203  0.37216517  0.15255252]\n",
      " [ 0.5484979  -0.430597    0.34724542  0.04113482]\n",
      " [ 0.522624    0.28485516  0.00297938  0.12154437]\n",
      " [ 0.22556773 -0.27479848  0.06086236 -0.19075096]\n",
      " [ 0.57514614  0.4417374  -0.4362241   0.1737367 ]\n",
      " [-0.14803278  0.01598388  0.23613018  0.19616961]\n",
      " [-0.00995207  0.11141294 -0.43796554  0.10492181]\n",
      " [ 0.3080092   0.3474084  -0.24372712 -0.4915411 ]\n",
      " [ 0.6009202  -0.4376558   0.4766692   0.1355403 ]\n",
      " [ 0.60161     0.49733016  0.5048913  -0.6830667 ]\n",
      " [ 0.62823844  0.5264207  -0.43567204  0.05987294]\n",
      " [-0.4477314   0.11181992 -0.10622957 -0.09693518]\n",
      " [ 0.06005758  0.40364558 -0.0392949  -0.4964552 ]\n",
      " [-0.23923656  0.49711064  0.5339876   0.2303302 ]\n",
      " [ 0.6568191   0.52576923  0.5639878   0.257516  ]\n",
      " [-0.5440019   0.46472573  0.00205296 -0.13045731]]\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for ep in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.train((state,action,next_state,reward,done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        print(\"s:\", state, \"a:\", action)\n",
    "        print(\"Episode: {}, Total reward: {}, eps: {}\".format(ep,total_reward,agent.eps))\n",
    "        env.render()\n",
    "    \n",
    "        \n",
    "        # Reuse=True... So if we run again, previously learned weights in q-table will be the starting point.\n",
    "        with tf.variable_scope(\"q_table\", reuse=True):\n",
    "            weights = agent.sess.run(tf.get_variable(\"kernel\"))\n",
    "            print(weights)\n",
    "        time.sleep(0.05)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

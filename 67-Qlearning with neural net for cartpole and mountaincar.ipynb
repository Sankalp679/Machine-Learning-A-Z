{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"MountainCar-v0\"\n",
    "env= gym.make(env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(2,)\n",
      "Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self, state_dim,action_size):\n",
    "        self.state_in = tf.placeholder(tf.float32,shape=[None,*state_dim])\n",
    "        self.action_in = tf.placeholder(tf.int32,shape=[None])\n",
    "        self.q_target_in = tf.placeholder(tf.float32,shape=[None])\n",
    "        # only o/p are one hot encoded as input is continous \n",
    "        action_one_hot = tf.one_hot(self.action_in,depth = action_size)\n",
    "        \n",
    "        self.hidden1=tf.layers.dense(self.state_in,100,activation=tf.nn.relu)\n",
    "        self.q_state=tf.layers.dense(self.hidden1,action_size,activation=None)\n",
    "        \n",
    "        self.q_state_action=tf.reduce_sum(tf.multiply(self.q_state,action_one_hot),axis=1)\n",
    "        \n",
    "        self.loss=tf.reduce_mean(tf.square(self.q_state_action-self.q_target_in))\n",
    "        \n",
    "        self.optimizer=tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
    "    \n",
    "    def update_model(self,session,state,action,q_target):\n",
    "        # main where weights gets updated\n",
    "        feed={self.state_in:state,self.action_in:action,self.q_target_in:q_target}\n",
    "        session.run(self.optimizer,feed_dict=feed)\n",
    "    \n",
    "    def get_q_state(self,session,state):\n",
    "        #run the model to get the state no weights gets updated \n",
    "        q_state=session.run(self.q_state,feed_dict={self.state_in:state})\n",
    "        return q_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to improve the model we are using replay buffer to get the optimal o/p\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,maxlen):\n",
    "        self.buffer=deque(maxlen=maxlen)\n",
    "    def add(self,experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        \n",
    "        sample_size = min(len(self.buffer),batch_size) # choose the smaller one\n",
    "        samples = random.choices(self.buffer,k=sample_size)\n",
    "        \n",
    "        return map(list,zip(*samples))  #maps all values of samples in list form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nwo the class to bind and call the above created classes\n",
    "class DQNAgent():\n",
    "    def __init__(self,env):\n",
    "        self.state_dim=env.observation_space.shape\n",
    "        self.action_size=env.action_space.n\n",
    "        # calling the neural network that we have created \n",
    "        self.q_network = QNetwork(self.state_dim,self.action_size)\n",
    "        self.replay_buffer=ReplayBuffer(maxlen=10000)\n",
    "        self.gamma=0.97\n",
    "        self.eps=1.0  #experience\n",
    "        self.sess=tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        q_state=self.q_network.get_q_state(self.sess,[state])\n",
    "        action_greedy=np.argmax(q_state)\n",
    "        action_random=np.random.randint(self.action_size)\n",
    "        action=action_random if random.random()<self.eps else action_greedy\n",
    "        return action\n",
    "    \n",
    "    def train(self,state,action,next_state,reward,done):\n",
    "        # buffer created \n",
    "        self.replay_buffer.add((state,action,next_state,reward,done))\n",
    "        # getting experince from the buffer\n",
    "        states,actions,next_states,rewards,dones=self.replay_buffer.sample(50)\n",
    "        q_next_states=self.q_network.get_q_state(self.sess,next_states)\n",
    "        q_next_states[dones]=np.zeros([self.action_size])\n",
    "        # q learing algo\n",
    "        q_targets= rewards+self.gamma*np.max(q_next_states,axis=1)\n",
    "        # now backprop to update table\n",
    "        self.q_network.update_model(self.sess,states,actions,q_targets)\n",
    "        if done:self.eps=max(0.1,0.99*self.eps)# to prevent eps vlaue below 0.1\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001637805CD48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "agent=DQNAgent(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode0, rewards=-200.0\n",
      "episode1, rewards=-200.0\n",
      "episode2, rewards=-200.0\n",
      "episode3, rewards=-200.0\n",
      "episode4, rewards=-200.0\n",
      "episode5, rewards=-200.0\n",
      "episode6, rewards=-200.0\n",
      "episode7, rewards=-200.0\n",
      "episode8, rewards=-200.0\n",
      "episode9, rewards=-200.0\n",
      "episode10, rewards=-200.0\n",
      "episode11, rewards=-200.0\n",
      "episode12, rewards=-200.0\n",
      "episode13, rewards=-200.0\n",
      "episode14, rewards=-200.0\n",
      "episode15, rewards=-200.0\n",
      "episode16, rewards=-200.0\n",
      "episode17, rewards=-200.0\n",
      "episode18, rewards=-200.0\n",
      "episode19, rewards=-200.0\n",
      "episode20, rewards=-200.0\n",
      "episode21, rewards=-200.0\n",
      "episode22, rewards=-200.0\n",
      "episode23, rewards=-200.0\n",
      "episode24, rewards=-200.0\n",
      "episode25, rewards=-200.0\n",
      "episode26, rewards=-200.0\n",
      "episode27, rewards=-200.0\n",
      "episode28, rewards=-200.0\n",
      "episode29, rewards=-200.0\n",
      "episode30, rewards=-200.0\n",
      "episode31, rewards=-200.0\n",
      "episode32, rewards=-200.0\n",
      "episode33, rewards=-200.0\n",
      "episode34, rewards=-200.0\n",
      "episode35, rewards=-200.0\n",
      "episode36, rewards=-200.0\n",
      "episode37, rewards=-200.0\n",
      "episode38, rewards=-200.0\n",
      "episode39, rewards=-200.0\n",
      "episode40, rewards=-200.0\n",
      "episode41, rewards=-200.0\n",
      "episode42, rewards=-200.0\n",
      "episode43, rewards=-200.0\n",
      "episode44, rewards=-200.0\n",
      "episode45, rewards=-200.0\n",
      "episode46, rewards=-200.0\n",
      "episode47, rewards=-200.0\n",
      "episode48, rewards=-200.0\n",
      "episode49, rewards=-200.0\n",
      "episode50, rewards=-200.0\n",
      "episode51, rewards=-200.0\n",
      "episode52, rewards=-200.0\n",
      "episode53, rewards=-200.0\n",
      "episode54, rewards=-200.0\n",
      "episode55, rewards=-200.0\n",
      "episode56, rewards=-200.0\n",
      "episode57, rewards=-200.0\n",
      "episode58, rewards=-200.0\n",
      "episode59, rewards=-200.0\n",
      "episode60, rewards=-200.0\n",
      "episode61, rewards=-200.0\n",
      "episode62, rewards=-200.0\n",
      "episode63, rewards=-200.0\n",
      "episode64, rewards=-200.0\n",
      "episode65, rewards=-200.0\n",
      "episode66, rewards=-200.0\n",
      "episode67, rewards=-200.0\n",
      "episode68, rewards=-200.0\n",
      "episode69, rewards=-200.0\n",
      "episode70, rewards=-200.0\n",
      "episode71, rewards=-200.0\n",
      "episode72, rewards=-200.0\n",
      "episode73, rewards=-200.0\n",
      "episode74, rewards=-200.0\n",
      "episode75, rewards=-200.0\n",
      "episode76, rewards=-200.0\n",
      "episode77, rewards=-200.0\n",
      "episode78, rewards=-200.0\n",
      "episode79, rewards=-200.0\n",
      "episode80, rewards=-200.0\n",
      "episode81, rewards=-200.0\n",
      "episode82, rewards=-200.0\n",
      "episode83, rewards=-200.0\n",
      "episode84, rewards=-200.0\n",
      "episode85, rewards=-200.0\n",
      "episode86, rewards=-200.0\n",
      "episode87, rewards=-200.0\n",
      "episode88, rewards=-200.0\n",
      "episode89, rewards=-200.0\n",
      "episode90, rewards=-200.0\n",
      "episode91, rewards=-200.0\n",
      "episode92, rewards=-200.0\n",
      "episode93, rewards=-200.0\n",
      "episode94, rewards=-200.0\n",
      "episode95, rewards=-200.0\n",
      "episode96, rewards=-200.0\n",
      "episode97, rewards=-200.0\n",
      "episode98, rewards=-200.0\n",
      "episode99, rewards=-200.0\n",
      "episode100, rewards=-200.0\n",
      "episode101, rewards=-200.0\n",
      "episode102, rewards=-200.0\n",
      "episode103, rewards=-200.0\n",
      "episode104, rewards=-169.0\n",
      "episode105, rewards=-200.0\n",
      "episode106, rewards=-200.0\n",
      "episode107, rewards=-200.0\n",
      "episode108, rewards=-200.0\n",
      "episode109, rewards=-200.0\n",
      "episode110, rewards=-200.0\n",
      "episode111, rewards=-200.0\n",
      "episode112, rewards=-200.0\n",
      "episode113, rewards=-200.0\n",
      "episode114, rewards=-200.0\n",
      "episode115, rewards=-200.0\n",
      "episode116, rewards=-200.0\n",
      "episode117, rewards=-200.0\n",
      "episode118, rewards=-200.0\n",
      "episode119, rewards=-200.0\n",
      "episode120, rewards=-200.0\n",
      "episode121, rewards=-200.0\n",
      "episode122, rewards=-200.0\n",
      "episode123, rewards=-200.0\n",
      "episode124, rewards=-200.0\n",
      "episode125, rewards=-200.0\n",
      "episode126, rewards=-200.0\n",
      "episode127, rewards=-200.0\n",
      "episode128, rewards=-200.0\n",
      "episode129, rewards=-200.0\n",
      "episode130, rewards=-200.0\n",
      "episode131, rewards=-200.0\n",
      "episode132, rewards=-200.0\n",
      "episode133, rewards=-200.0\n",
      "episode134, rewards=-200.0\n",
      "episode135, rewards=-200.0\n",
      "episode136, rewards=-200.0\n",
      "episode137, rewards=-200.0\n",
      "episode138, rewards=-200.0\n",
      "episode139, rewards=-200.0\n",
      "episode140, rewards=-200.0\n",
      "episode141, rewards=-200.0\n",
      "episode142, rewards=-200.0\n",
      "episode143, rewards=-200.0\n",
      "episode144, rewards=-200.0\n",
      "episode145, rewards=-200.0\n",
      "episode146, rewards=-200.0\n",
      "episode147, rewards=-200.0\n",
      "episode148, rewards=-200.0\n",
      "episode149, rewards=-200.0\n",
      "episode150, rewards=-200.0\n",
      "episode151, rewards=-200.0\n",
      "episode152, rewards=-200.0\n",
      "episode153, rewards=-200.0\n",
      "episode154, rewards=-200.0\n",
      "episode155, rewards=-200.0\n",
      "episode156, rewards=-200.0\n",
      "episode157, rewards=-200.0\n",
      "episode158, rewards=-200.0\n",
      "episode159, rewards=-200.0\n",
      "episode160, rewards=-200.0\n",
      "episode161, rewards=-200.0\n",
      "episode162, rewards=-200.0\n",
      "episode163, rewards=-200.0\n",
      "episode164, rewards=-200.0\n",
      "episode165, rewards=-200.0\n",
      "episode166, rewards=-200.0\n",
      "episode167, rewards=-200.0\n",
      "episode168, rewards=-200.0\n",
      "episode169, rewards=-200.0\n",
      "episode170, rewards=-200.0\n",
      "episode171, rewards=-200.0\n",
      "episode172, rewards=-200.0\n",
      "episode173, rewards=-200.0\n",
      "episode174, rewards=-200.0\n",
      "episode175, rewards=-167.0\n",
      "episode176, rewards=-125.0\n",
      "episode177, rewards=-200.0\n",
      "episode178, rewards=-200.0\n",
      "episode179, rewards=-200.0\n",
      "episode180, rewards=-200.0\n",
      "episode181, rewards=-200.0\n",
      "episode182, rewards=-189.0\n",
      "episode183, rewards=-200.0\n",
      "episode184, rewards=-200.0\n",
      "episode185, rewards=-177.0\n",
      "episode186, rewards=-200.0\n",
      "episode187, rewards=-200.0\n",
      "episode188, rewards=-200.0\n",
      "episode189, rewards=-200.0\n",
      "episode190, rewards=-200.0\n",
      "episode191, rewards=-166.0\n",
      "episode192, rewards=-200.0\n",
      "episode193, rewards=-200.0\n",
      "episode194, rewards=-200.0\n",
      "episode195, rewards=-200.0\n",
      "episode196, rewards=-171.0\n",
      "episode197, rewards=-200.0\n",
      "episode198, rewards=-200.0\n",
      "episode199, rewards=-200.0\n",
      "episode200, rewards=-163.0\n",
      "episode201, rewards=-200.0\n",
      "episode202, rewards=-200.0\n",
      "episode203, rewards=-200.0\n",
      "episode204, rewards=-200.0\n",
      "episode205, rewards=-200.0\n",
      "episode206, rewards=-200.0\n",
      "episode207, rewards=-200.0\n",
      "episode208, rewards=-200.0\n",
      "episode209, rewards=-182.0\n",
      "episode210, rewards=-200.0\n",
      "episode211, rewards=-189.0\n",
      "episode212, rewards=-200.0\n",
      "episode213, rewards=-200.0\n",
      "episode214, rewards=-200.0\n",
      "episode215, rewards=-200.0\n",
      "episode216, rewards=-200.0\n",
      "episode217, rewards=-200.0\n",
      "episode218, rewards=-200.0\n",
      "episode219, rewards=-200.0\n",
      "episode220, rewards=-200.0\n",
      "episode221, rewards=-200.0\n",
      "episode222, rewards=-196.0\n",
      "episode223, rewards=-200.0\n",
      "episode224, rewards=-200.0\n",
      "episode225, rewards=-183.0\n",
      "episode226, rewards=-200.0\n",
      "episode227, rewards=-200.0\n",
      "episode228, rewards=-200.0\n",
      "episode229, rewards=-170.0\n",
      "episode230, rewards=-200.0\n",
      "episode231, rewards=-200.0\n",
      "episode232, rewards=-176.0\n",
      "episode233, rewards=-200.0\n",
      "episode234, rewards=-169.0\n",
      "episode235, rewards=-200.0\n",
      "episode236, rewards=-200.0\n",
      "episode237, rewards=-200.0\n",
      "episode238, rewards=-200.0\n",
      "episode239, rewards=-200.0\n",
      "episode240, rewards=-200.0\n",
      "episode241, rewards=-200.0\n",
      "episode242, rewards=-200.0\n",
      "episode243, rewards=-200.0\n",
      "episode244, rewards=-200.0\n",
      "episode245, rewards=-163.0\n",
      "episode246, rewards=-200.0\n",
      "episode247, rewards=-200.0\n",
      "episode248, rewards=-200.0\n",
      "episode249, rewards=-200.0\n",
      "episode250, rewards=-200.0\n",
      "episode251, rewards=-200.0\n",
      "episode252, rewards=-200.0\n",
      "episode253, rewards=-163.0\n",
      "episode254, rewards=-200.0\n",
      "episode255, rewards=-156.0\n",
      "episode256, rewards=-200.0\n",
      "episode257, rewards=-167.0\n",
      "episode258, rewards=-180.0\n",
      "episode259, rewards=-163.0\n",
      "episode260, rewards=-194.0\n",
      "episode261, rewards=-194.0\n",
      "episode262, rewards=-173.0\n",
      "episode263, rewards=-188.0\n",
      "episode264, rewards=-157.0\n",
      "episode265, rewards=-170.0\n",
      "episode266, rewards=-200.0\n",
      "episode267, rewards=-176.0\n",
      "episode268, rewards=-173.0\n",
      "episode269, rewards=-178.0\n",
      "episode270, rewards=-200.0\n",
      "episode271, rewards=-200.0\n",
      "episode272, rewards=-200.0\n",
      "episode273, rewards=-200.0\n",
      "episode274, rewards=-200.0\n",
      "episode275, rewards=-200.0\n",
      "episode276, rewards=-159.0\n",
      "episode277, rewards=-200.0\n",
      "episode278, rewards=-169.0\n",
      "episode279, rewards=-197.0\n",
      "episode280, rewards=-200.0\n",
      "episode281, rewards=-173.0\n",
      "episode282, rewards=-175.0\n",
      "episode283, rewards=-200.0\n",
      "episode284, rewards=-162.0\n",
      "episode285, rewards=-200.0\n",
      "episode286, rewards=-200.0\n",
      "episode287, rewards=-184.0\n",
      "episode288, rewards=-194.0\n",
      "episode289, rewards=-200.0\n",
      "episode290, rewards=-200.0\n",
      "episode291, rewards=-159.0\n",
      "episode292, rewards=-200.0\n",
      "episode293, rewards=-200.0\n",
      "episode294, rewards=-162.0\n",
      "episode295, rewards=-178.0\n",
      "episode296, rewards=-200.0\n",
      "episode297, rewards=-158.0\n",
      "episode298, rewards=-159.0\n",
      "episode299, rewards=-163.0\n",
      "episode300, rewards=-169.0\n",
      "episode301, rewards=-173.0\n",
      "episode302, rewards=-147.0\n",
      "episode303, rewards=-162.0\n",
      "episode304, rewards=-159.0\n",
      "episode305, rewards=-200.0\n",
      "episode306, rewards=-168.0\n",
      "episode307, rewards=-161.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode308, rewards=-181.0\n",
      "episode309, rewards=-189.0\n",
      "episode310, rewards=-200.0\n",
      "episode311, rewards=-170.0\n",
      "episode312, rewards=-170.0\n",
      "episode313, rewards=-164.0\n",
      "episode314, rewards=-163.0\n",
      "episode315, rewards=-200.0\n",
      "episode316, rewards=-163.0\n",
      "episode317, rewards=-177.0\n",
      "episode318, rewards=-162.0\n",
      "episode319, rewards=-200.0\n",
      "episode320, rewards=-190.0\n",
      "episode321, rewards=-170.0\n",
      "episode322, rewards=-163.0\n",
      "episode323, rewards=-186.0\n",
      "episode324, rewards=-200.0\n",
      "episode325, rewards=-172.0\n",
      "episode326, rewards=-200.0\n",
      "episode327, rewards=-200.0\n",
      "episode328, rewards=-200.0\n",
      "episode329, rewards=-159.0\n",
      "episode330, rewards=-164.0\n",
      "episode331, rewards=-200.0\n",
      "episode332, rewards=-200.0\n",
      "episode333, rewards=-200.0\n",
      "episode334, rewards=-200.0\n",
      "episode335, rewards=-200.0\n",
      "episode336, rewards=-200.0\n",
      "episode337, rewards=-200.0\n",
      "episode338, rewards=-200.0\n",
      "episode339, rewards=-152.0\n",
      "episode340, rewards=-200.0\n",
      "episode341, rewards=-162.0\n",
      "episode342, rewards=-168.0\n",
      "episode343, rewards=-200.0\n",
      "episode344, rewards=-200.0\n",
      "episode345, rewards=-197.0\n",
      "episode346, rewards=-193.0\n",
      "episode347, rewards=-200.0\n",
      "episode348, rewards=-169.0\n",
      "episode349, rewards=-159.0\n",
      "episode350, rewards=-150.0\n",
      "episode351, rewards=-164.0\n",
      "episode352, rewards=-171.0\n",
      "episode353, rewards=-158.0\n",
      "episode354, rewards=-161.0\n",
      "episode355, rewards=-158.0\n",
      "episode356, rewards=-155.0\n",
      "episode357, rewards=-142.0\n",
      "episode358, rewards=-167.0\n",
      "episode359, rewards=-165.0\n",
      "episode360, rewards=-149.0\n",
      "episode361, rewards=-200.0\n",
      "episode362, rewards=-145.0\n",
      "episode363, rewards=-169.0\n",
      "episode364, rewards=-174.0\n",
      "episode365, rewards=-180.0\n",
      "episode366, rewards=-200.0\n",
      "episode367, rewards=-200.0\n",
      "episode368, rewards=-160.0\n",
      "episode369, rewards=-168.0\n",
      "episode370, rewards=-121.0\n",
      "episode371, rewards=-200.0\n",
      "episode372, rewards=-177.0\n",
      "episode373, rewards=-192.0\n",
      "episode374, rewards=-155.0\n",
      "episode375, rewards=-165.0\n",
      "episode376, rewards=-200.0\n",
      "episode377, rewards=-200.0\n",
      "episode378, rewards=-175.0\n",
      "episode379, rewards=-171.0\n",
      "episode380, rewards=-168.0\n",
      "episode381, rewards=-200.0\n",
      "episode382, rewards=-182.0\n",
      "episode383, rewards=-165.0\n",
      "episode384, rewards=-136.0\n",
      "episode385, rewards=-200.0\n",
      "episode386, rewards=-200.0\n",
      "episode387, rewards=-172.0\n",
      "episode388, rewards=-112.0\n",
      "episode389, rewards=-200.0\n",
      "episode390, rewards=-159.0\n",
      "episode391, rewards=-200.0\n",
      "episode392, rewards=-200.0\n",
      "episode393, rewards=-166.0\n",
      "episode394, rewards=-162.0\n",
      "episode395, rewards=-142.0\n",
      "episode396, rewards=-157.0\n",
      "episode397, rewards=-145.0\n",
      "episode398, rewards=-161.0\n",
      "episode399, rewards=-147.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 400\n",
    "for ep in range(num_episodes):\n",
    "    state=env.reset()\n",
    "    total_reward=0\n",
    "    done=False\n",
    "    while not done:\n",
    "        action=agent.get_action(state)\n",
    "        next_state,reward,done,info=env.step(action)\n",
    "        agent.train(state,action,next_state,reward,done)\n",
    "        env.render()\n",
    "        total_reward+=reward\n",
    "        state=next_state\n",
    "        # Reuse=True... So if we run again, previously learned weights in q-table will be the starting point.\n",
    "#         tf.variable_scope(\"q_table\", reuse=True)\n",
    "    print(f\"episode{ep}, rewards={total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
